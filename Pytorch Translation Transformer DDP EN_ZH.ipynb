{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9352304,"sourceType":"datasetVersion","datasetId":5669200},{"sourceId":9385091,"sourceType":"datasetVersion","datasetId":5672793},{"sourceId":9454280,"sourceType":"datasetVersion","datasetId":5746160},{"sourceId":198084063,"sourceType":"kernelVersion"},{"sourceId":202162584,"sourceType":"kernelVersion"},{"sourceId":202165004,"sourceType":"kernelVersion"},{"sourceId":205085179,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bcgggg/pytorch-translation-transformer-ddp-en-zh?scriptVersionId=202244328\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install torch pandas datasets torchtext spacy\n!python -m spacy download zh_core_web_sm\n!python -m spacy download en_core_web_sm\n\nimport torch\nimport torchtext\n\nprint(torch.__version__)\nprint(torchtext.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T12:18:38.78702Z","iopub.execute_input":"2024-10-19T12:18:38.787313Z","iopub.status.idle":"2024-10-19T12:19:34.435188Z","shell.execute_reply.started":"2024-10-19T12:18:38.787287Z","shell.execute_reply":"2024-10-19T12:19:34.434242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rsync -av --exclude='pytorch-translation-transformer-ddp-en-zh' /kaggle/input/* /kaggle/working/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rsync -av /kaggle/input/pytorch-translation-transformer-ddp-en-zh/output/* /kaggle/working/output\n# !rsync -av /kaggle/input/chianbcg-translation-en-zh/output/* /kaggle/working/output\n# !rsync -av /kaggle/input/bcggggg-translation-en-zh/output/* /kaggle/working/output","metadata":{"execution":{"iopub.status.busy":"2024-10-20T09:38:11.181228Z","iopub.execute_input":"2024-10-20T09:38:11.18171Z","iopub.status.idle":"2024-10-20T09:39:44.379952Z","shell.execute_reply.started":"2024-10-20T09:38:11.181673Z","shell.execute_reply":"2024-10-20T09:39:44.378682Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.environ['BATCH_SIZE'] = \"50\"\nos.environ['DATA_TRAINING_ITER'] = \"2\"\nos.environ['WORK_DIRECTORY_INPUT'] = '/kaggle/working/input'\n# os.environ['EXPORT_ONNX'] = \"True\"\n# os.environ['INFERENCE_ONLY'] = \"True\"\n# os.environ['TEST_ONLY'] = \"True\"","metadata":{"execution":{"iopub.status.busy":"2024-10-16T16:00:18.016599Z","iopub.execute_input":"2024-10-16T16:00:18.017435Z","iopub.status.idle":"2024-10-16T16:00:18.265838Z","shell.execute_reply.started":"2024-10-16T16:00:18.017404Z","shell.execute_reply":"2024-10-16T16:00:18.264913Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile ddp.py\n############################### Utils #######################################\nimport json\nfrom pathlib import Path\nfrom timeit import default_timer as timer\n\nxprint = print\ndef print(*args, **kwargs):\n    xprint(f'[Rank {RANK}]{args[0]}', *args[1:], **kwargs)\n    \ndef saveText(txt: str, filePath, option = {}):\n    file = Path(filePath)\n    file.parent.mkdir(parents=True, exist_ok=True)\n    file.write_text(txt)\n    del file\n    print(f\"Saved text: {filePath}{(', content:' + txt) if option.get('printContent', False) else ''}\")\n\ndef saveObject(obj: any, filePath):\n    torch.save(obj, f'{filePath}')\n    del obj\n    print(f'Saved object: {filePath}')\n    \ndef saveJSON(dicts: dict, filePath, option = {}):\n    saveText(json.dumps(dicts), filePath, option)\n    \ndef readText(filePath):\n    text = ''\n    if os.path.exists(filePath):\n      with open(filePath, 'r', encoding=\"utf-8\") as f:\n        text = f.read()\n    return text\n    \ndef readJSON(filePath):\n    status = {}\n    if os.path.exists(filePath):\n      with open(filePath, 'r', encoding=\"utf-8\") as f:\n        status = json.load(f)\n    return status\n\ndef appendText(filePath: str, text: str):\n    saveText(f'{readText(filePath)}{text}\\n', filePath)\n\n############################### Prepare hyperparameters and model #######################################\nimport os\nimport torch\nimport torch.distributed as dist\nimport pandas as pd\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n\nSRC_LANGUAGE = os.getenv(\"SRC_LANGUAGE\", 'en')\nTGT_LANGUAGE = os.getenv(\"TGT_LANGUAGE\", 'zh')\nWORK_DIRECTORY_INPUT = os.getenv(\"WORK_DIRECTORY_INPUT\", '/kaggle/input/pytorch-translation-transformer-ddp-en-zh/input')\nWORK_DIRECTORY_OUTPUT = os.getenv(\"WORK_DIRECTORY_OUTPUT\", '/kaggle/working/output')\n\nBATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 128))\nNUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", 1))\nDATA_LEN = int(os.getenv(\"DATA_LEN\", '0'))\nDATA_LEN_VAL_PERT = float(os.getenv(\"DATA_LEN_VAL_PERT\", 0.001))\nDATA_TRAINING_ITER = int(os.getenv(\"DATA_TRAINING_ITER\", '1'))\nVOCAB_FILE = f'{WORK_DIRECTORY_INPUT}/vocab-10m/vocab-9999621-20-72696-82723.pt'\nVOCAB_MIN_FREQ = int(os.getenv(\"VOCAB_MIN_FREQ\", 40))\n\nNHEAD = int(os.getenv(\"NHEAD\", 8))\nEMB_SIZE = int(os.getenv(\"EMB_SIZE\", 512))\nFFN_HID_DIM = int(os.getenv(\"FFN_HID_DIM\", 2048))\nNUM_ENCODER_LAYERS = int(os.getenv(\"NUM_ENCODER_LAYERS\", 6))\nNUM_DECODER_LAYERS = int(os.getenv(\"NUM_DECODER_LAYERS\", 6))\nDROPOUT_RATE = float(os.getenv(\"DROPOUT_RATE\", 0.1))\n\nTEST_ONLY = os.getenv(\"TEST_ONLY\", 'False') == 'True'\nINFERENCE_ONLY = os.getenv(\"INFERENCE_ONLY\", 'False') == 'True'\nEXPORT_ONNX = os.getenv(\"EXPORT_ONNX\", 'False') == 'True'\nGRAD_ACCUM_ITER = int(os.getenv(\"GRAD_ACCUM_ITER\", '4'))\nEARLY_STOPING_PATIENCE = int(os.getenv(\"EARLY_STOPING_PATIENCE\", 6))\nRANK = int(os.getenv(\"RANK\", 0))\nWORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\nLOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", 0))\n\nGPU_COUNT = torch.cuda.device_count()\nDDP_ENABLED = WORLD_SIZE > 1\nAMP_ENABLED = GPU_COUNT > 0\nDEVICE = torch.device(f'cuda:{LOCAL_RANK % GPU_COUNT}' if GPU_COUNT > 0 else 'cpu')\n\nDATA_TRAINING_ITER_INDEX = (readJSON(f'{WORK_DIRECTORY_OUTPUT}/train/params.json').get(\"DATA_TRAINING_ITER_INDEX\", -1) + 1) % DATA_TRAINING_ITER\nIS_DATA_TRAINING_ITER_MODE = (DATA_TRAINING_ITER_INDEX + 1 < DATA_TRAINING_ITER and not INFERENCE_ONLY)\n\nif DATA_TRAINING_ITER > 1:\n    NUM_EPOCHS = 1\n    print(\"Fallback NUM_EPOCHES to 1 when DATA_TRAINING_ITER is gt than 1\")\n\nprint(f'DDP_ENABLED: {DDP_ENABLED}, WORLD_SIZE: {WORLD_SIZE}, RANK: {RANK}, LOCAL_RANK: {LOCAL_RANK}, GPU_COUNT: {GPU_COUNT}, DEVICE: {DEVICE}')\n\nif not INFERENCE_ONLY:\n    ############################### Prepare dataset #######################################\n    df_train_6m_origin = pd.read_json(f'{WORK_DIRECTORY_INPUT}/en-zh-6m/translation2019zh_train.json', lines=True)\n    df_valid_6m_origin = pd.read_json(f'{WORK_DIRECTORY_INPUT}/en-zh-6m/translation2019zh_valid.json', lines=True)\n    df_origin_6m = pd.concat([df_train_6m_origin, df_valid_6m_origin], ignore_index=True)\n    df_origin_6m = df_origin_6m.rename(columns={\"english\": \"en\", \"chinese\": \"zh\"})\n    df_train_10m_origin_en = pd.read_csv(f'{WORK_DIRECTORY_INPUT}/en-zh-10m/train.en', engine='python', sep='\\r\\t', header=None, keep_default_na=False, names=['en'])\n    df_train_10m_origin_zh = pd.read_csv(f'{WORK_DIRECTORY_INPUT}/en-zh-10m/train.zh', engine='python', sep='\\r\\t', header=None, keep_default_na=False, names=['zh'])\n    df_origin_10m = pd.concat([df_train_10m_origin_en, df_train_10m_origin_zh], axis=1)\n    df_origin = pd.concat([df_origin_6m, df_origin_10m], ignore_index=True)\n    df = df_origin[df_origin['en'].map(len) <= 300].reset_index(drop=True)\n\n    if RANK == 0:\n        print(df)\n    \n    if DATA_LEN == 0:\n        DATA_LEN = df.shape[0]\n\n    num_val_sample = int(DATA_LEN_VAL_PERT * DATA_LEN)\n    num_train_samples = DATA_LEN - 2 * num_val_sample\n    num_train_iter_samples = int(num_train_samples / DATA_TRAINING_ITER)\n\n    train_ds = df[num_train_iter_samples * DATA_TRAINING_ITER_INDEX: num_train_iter_samples * (DATA_TRAINING_ITER_INDEX + 1) if IS_DATA_TRAINING_ITER_MODE else num_train_samples]\n    val_ds = df[num_train_samples: num_train_samples + num_val_sample]\n    test_ds = df[num_train_samples + num_val_sample: num_train_samples + 2 * num_val_sample]\n\n    print(f'Train shape: {train_ds.shape}')\n    if RANK == 0:\n        print(train_ds)\n    print(f'  Val shape: {val_ds.shape}')\n    print(f' Test shape: {test_ds.shape}')\n    del df_train_6m_origin, df_valid_6m_origin, df_origin_6m, df_train_10m_origin_en, df_train_10m_origin_zh, df_origin_10m, df_origin, df\n\n\n############################### Prepare vocabulary #######################################\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntoken_transform = {\n 'en': get_tokenizer('spacy', language='en_core_web_sm'),\n 'zh': get_tokenizer('spacy', language='zh_core_web_sm')\n}\n\nvocab_transform = {}\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\nif os.path.exists(VOCAB_FILE):\n    vocab_transform = torch.load(VOCAB_FILE)\n    print(f'Loaded voab file: {VOCAB_FILE}')\n\nelse :\n    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n        vocab_transform[ln] = build_vocab_from_iterator(train_ds[ln].transform(lambda x: token_transform[ln](x)),\n                              min_freq=VOCAB_MIN_FREQ,\n                              specials=special_symbols,\n                              special_first=True)\n        # Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n        # If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n        vocab_transform[ln].set_default_index(UNK_IDX)\n    torch.save(vocab_transform, VOCAB_FILE)\n    print(f'Saved voab file: {VOCAB_FILE}')\n\n# if RANK == 0:\n#     for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n#         print(f'Vocab {ln}: {dict(enumerate(vocab_transform[ln].get_itos()))}')\n\n############################### Prepare model #######################################\n\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nimport math\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int,\n                 dropout: float):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                tgt: Tensor):\n        \n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt, DEVICE)\n        \n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, src_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)\n    \n############################### Prepare masking #######################################\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt, device):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n    \n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(device)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool).to(device)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\n\n############################### Prepare training dataset #######################################\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset\nfrom typing import List\n\n# helper function to club together sequential operations\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    return torch.cat((torch.tensor([BOS_IDX]),\n                      torch.tensor(token_ids),\n                      torch.tensor([EOS_IDX])))\n\n# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n                                               vocab_transform[ln], #Numericalization\n                                               tensor_transform) # Add BOS/EOS and create tensor\n\n\n# function to collate data samples into batch tensors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch\n\nclass PandasDataset(Dataset):\n  def __init__(self, dataframe):\n    self.dataframe = dataframe\n\n  def __getitem__(self, index):\n    feature = self.dataframe[index: index + 1][SRC_LANGUAGE].to_numpy()[0]\n    label = self.dataframe[index: index + 1][TGT_LANGUAGE].to_numpy()[0]\n    return feature, label\n\n  def __len__(self):\n    return len(self.dataframe)\n\n\n############################### Prepare hyperparameters and model #######################################\nimport torch.distributed as dist\n\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, \n                                 NUM_DECODER_LAYERS, \n                                 EMB_SIZE, \n                                 NHEAD, \n                                 SRC_VOCAB_SIZE, \n                                 TGT_VOCAB_SIZE, \n                                 FFN_HID_DIM, \n                                 DROPOUT_RATE)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\nscaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)\n\n\n############################### Loading weights #######################################\nfrom datetime import datetime\nimport time\nimport os.path\nimport pytz\nimport json\n\ngetNowInLocal = lambda : str(datetime.now(pytz.timezone('Asia/Shanghai')))\n\nval_loss_prev = float('inf')\nval_loss_increased_count = 0\n\nweightFoler = 'best' if DATA_TRAINING_ITER_INDEX == 0 else 'train'\nweightsFilePath =  f'{WORK_DIRECTORY_OUTPUT}/{weightFoler}/model_weights.pth' \nif os.path.exists(weightsFilePath):\n    checkpoint = torch.load(weightsFilePath, map_location=torch.device(DEVICE))\n    transformer.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    scaler.load_state_dict(checkpoint['scaler'])\n    val_loss = readJSON(f'{WORK_DIRECTORY_OUTPUT}/{weightFoler}/status.json').get('val_loss_prev', float('inf'))\n    print(f'Loaded weights file, type: {weightFoler}, val_loss: {val_loss}, file: {weightsFilePath}.')\n    del weightsFilePath, checkpoint\n\nstatusFilePath =  f'{WORK_DIRECTORY_OUTPUT}/best/status.json'\nif os.path.exists(statusFilePath):\n    status = readJSON(statusFilePath)\n    val_loss_prev = status.get('val_loss_prev', float('inf'))\n    print(f'Loaded status file from {statusFilePath}, val_loss_prev: {val_loss_prev}.')\n    del statusFilePath, status\n\n\n############################### ENABLE DDP #######################################\nif DDP_ENABLED:\n    dist.init_process_group(backend=(\"nccl\" if GPU_COUNT > 0 else 'gloo'))\n    transformer = torch.nn.parallel.DistributedDataParallel(transformer.to(DEVICE), device_ids=([LOCAL_RANK] if DEVICE.type != 'cpu' else None), output_device=(LOCAL_RANK if DEVICE.type != 'cpu' else None))\n    \n    if not INFERENCE_ONLY:\n        train_ds_sampler =  torch.utils.data.distributed.DistributedSampler(PandasDataset(train_ds))\n        val_ds_sampler =  torch.utils.data.distributed.DistributedSampler(PandasDataset(val_ds))\n\n############################### Prepare traning and validation epoch #######################################\nif not INFERENCE_ONLY and not TEST_ONLY:\n\n    from torch.utils.data import DataLoader\n    import math\n\n    def train_epoch(model, optimizer, epoch, scaler):\n        if not DEVICE.type == 'cpu':\n            torch.cuda.reset_peak_memory_stats(DEVICE)\n        model.train()\n        losses = 0\n        losses_ga = 0\n        batch = 0\n        train_dataloader = DataLoader(PandasDataset(train_ds), batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=train_ds_sampler if DDP_ENABLED else None)\n        if DDP_ENABLED:\n            train_dataloader.sampler.set_epoch(epoch)\n\n        for src, tgt in train_dataloader:\n            batch += 1\n            start_time = timer()\n            src = src.to(DEVICE)\n            tgt = tgt.to(DEVICE)\n\n            tgt_input = tgt[:-1, :]\n\n            with torch.autocast(device_type=DEVICE.type, enabled=AMP_ENABLED):\n                logits = model(src, tgt_input)\n\n            tgt_out = tgt[1:, :]\n            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n            loss /= GRAD_ACCUM_ITER\n            losses_ga += loss.item()\n\n            scaler.scale(loss).backward()\n\n            if (batch % GRAD_ACCUM_ITER == 0) or (batch == len(train_dataloader)):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                losses += losses_ga * GRAD_ACCUM_ITER\n\n            end_time = timer()\n\n            if RANK == 0 and ((batch % GRAD_ACCUM_ITER == 0) or (batch == len(train_dataloader))):\n                print(f'[Tra]Epoch: {epoch}, Batch: {batch}/{len(train_dataloader)}, Data: {((batch - 1) * BATCH_SIZE + src.shape[1])}/{math.floor(train_ds.shape[0] / WORLD_SIZE)}, Train loss: {losses_ga:.3f}, ETA: {((len(train_dataloader) - batch) * (end_time - start_time)):.3f}s, Memory allocated: {torch.cuda.memory_allocated(DEVICE) // (1024 ** 3)} GB/{torch.cuda.max_memory_allocated(DEVICE) // (1024 ** 3)} GB, Datetime:{getNowInLocal()}', end=\"\\r\" if batch != len(train_dataloader) else '\\n', flush=False if batch == 1 else True)\n\n            if (batch % GRAD_ACCUM_ITER == 0) or (batch == len(train_dataloader)):\n                losses_ga = 0\n\n            del src, tgt, start_time, tgt_input, logits, tgt_out, loss, end_time\n\n        return losses / len(train_dataloader)\n\n    def evaluate(model, epoch):\n        model.eval()\n        losses = 0\n        losses_ga = 0\n        batch = 0\n        val_dataloader = DataLoader(PandasDataset(val_ds), batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=val_ds_sampler if DDP_ENABLED else None)\n        if DDP_ENABLED:\n            val_dataloader.sampler.set_epoch(epoch)\n\n        for src, tgt in val_dataloader:\n            if not DEVICE.type == 'cpu':\n                torch.cuda.reset_peak_memory_stats(DEVICE)\n            batch += 1\n            start_time = timer()\n            src = src.to(DEVICE)\n            tgt = tgt.to(DEVICE)\n\n            tgt_input = tgt[:-1, :]\n\n            with torch.autocast(device_type=DEVICE.type, enabled=AMP_ENABLED):\n                logits = model(src, tgt_input)\n\n            tgt_out = tgt[1:, :]\n            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n            loss /= GRAD_ACCUM_ITER\n            losses_ga += loss.item()\n            end_time = timer()\n\n            if (batch % GRAD_ACCUM_ITER == 0) or (batch == len(val_dataloader)):\n                losses += losses_ga * GRAD_ACCUM_ITER\n\n            if RANK == 0 and ((batch % GRAD_ACCUM_ITER == 0) or (batch == len(val_dataloader))):\n                print(f'[Val]Epoch: {epoch}, Batch: {batch}/{len(val_dataloader)}, Data: {((batch - 1) * BATCH_SIZE + src.shape[1])}/{math.floor(val_ds.shape[0] / WORLD_SIZE)}, Train loss: {losses_ga:.3f}, ETA: {((len(val_dataloader) - batch) * (end_time - start_time)):.3f}s, Memory allocated: {torch.cuda.memory_allocated(DEVICE) // (1024 ** 3)} GB/{torch.cuda.max_memory_allocated(DEVICE) // (1024 ** 3)} GB, Datetime:{getNowInLocal()}', end=\"\\r\" if batch != len(val_dataloader) else '\\n', flush=False if batch == 1 else True)\n\n            if (batch % GRAD_ACCUM_ITER == 0) or (batch == len(val_dataloader)):\n                losses_ga = 0\n\n            del src, tgt, start_time, tgt_input, logits, tgt_out, loss, end_time\n\n        return losses / len(val_dataloader)\n\n\n    ############################### Training model #######################################\n    from timeit import default_timer as timer\n    import os.path\n    import shutil\n    \n    for epoch in range(1, NUM_EPOCHS+1):\n        start_time = timer()\n        train_loss = train_epoch(transformer, optimizer, epoch, scaler)\n        end_time = timer()\n        start_time_val = timer()\n        val_loss = evaluate(transformer, epoch)\n        end_time_val = timer()\n\n        if RANK == 0:\n            log = f\"[Sum]Epoch: {epoch}, Data: {train_ds.shape[0] / WORLD_SIZE}, DATA_TRAINING_ITER: {DATA_TRAINING_ITER_INDEX + 1}/{DATA_TRAINING_ITER}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s, Val Epoch time = {(end_time_val - start_time_val):.3f}s, Memory allocated: {torch.cuda.memory_allocated(DEVICE) // (1024 ** 3)} GB/{torch.cuda.max_memory_allocated(DEVICE) // (1024 ** 3)} GB, Datetime:{getNowInLocal()}\"\n            print(log)\n            pathOfTrainlogFile = f\"{WORK_DIRECTORY_OUTPUT}/train/history.txt\"\n            pathOfBestlogFile = f\"{WORK_DIRECTORY_OUTPUT}/best/history.txt\"\n            if DATA_TRAINING_ITER_INDEX == 0:\n                appendText(pathOfTrainlogFile, f'{readText(pathOfBestlogFile)}{log}')\n            else:\n                appendText(pathOfTrainlogFile, log)\n            \n            weights = {\n                \"model\": transformer.module.state_dict() if DDP_ENABLED else transformer.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scaler\": scaler.state_dict()\n            }\n            saveObject(weights, f'{WORK_DIRECTORY_OUTPUT}/train/model_weights.pth')\n            \n            status = {\n                \"val_loss_prev\": val_loss, \n                \"datatime\": time.ctime()\n            }\n            saveJSON(status, f'{WORK_DIRECTORY_OUTPUT}/train/status.json', { \"printContent\": True })   \n\n            params = {\n                \"GRAD_ACCUM_ITER\":  GRAD_ACCUM_ITER,\n                \"NUM_EPOCHS\": NUM_EPOCHS,\n                \"DATA_LEN\": DATA_LEN,\n                \"VOCAB_FILE\": VOCAB_FILE,\n                \"VOCAB_MIN_FREQ\": VOCAB_MIN_FREQ,\n                \"EMB_SIZE\": EMB_SIZE,\n                \"NHEAD\": NHEAD,\n                \"FFN_HID_DIM\": FFN_HID_DIM,\n                \"BATCH_SIZE\": BATCH_SIZE,\n                \"NUM_ENCODER_LAYERS\": NUM_ENCODER_LAYERS,\n                \"NUM_DECODER_LAYERS\":NUM_DECODER_LAYERS,\n                \"EARLY_STOPING_PATIENCE\": EARLY_STOPING_PATIENCE,\n                \"WORLD_SIZE\": WORLD_SIZE,\n                \"GPU_COUNT\": GPU_COUNT,\n                \"DDP_ENABLED\": DDP_ENABLED,\n                \"DATA_TRAINING_ITER\": DATA_TRAINING_ITER,\n                \"DATA_TRAINING_ITER_INDEX\": DATA_TRAINING_ITER_INDEX\n            }\n            saveJSON(params, f'{WORK_DIRECTORY_OUTPUT}/train/params.json')\n\n            if not IS_DATA_TRAINING_ITER_MODE and val_loss < val_loss_prev:\n                shutil.rmtree(f'{WORK_DIRECTORY_OUTPUT}/best', ignore_errors=True)\n                os.rename(f'{WORK_DIRECTORY_OUTPUT}/train', f'{WORK_DIRECTORY_OUTPUT}/best')\n\n        if not IS_DATA_TRAINING_ITER_MODE:\n            if val_loss < val_loss_prev:\n                val_loss_increased_count = 0\n                val_loss_prev = val_loss\n\n            else:\n                val_loss_increased_count += 1\n\n            if val_loss_increased_count == EARLY_STOPING_PATIENCE:\n              print(f'Stop training as val_loss is increased {EARLY_STOPING_PATIENCE} times.')\n              break\n            \nif EXPORT_ONNX and not IS_DATA_TRAINING_ITER_MODE:\n    src = text_transform[SRC_LANGUAGE]('Hello world!').view(-1, 1)\n    tgt = text_transform[TGT_LANGUAGE]('<bos>').view(-1, 1)\n    torch.onnx.export(transformer.module if DDP_ENABLED else transformer, \n                      (src, tgt),\n                      f\"{WORK_DIRECTORY_OUTPUT}/pytorch_translation_{SRC_LANGUAGE}_{TGT_LANGUAGE}.onnx\", \n                      verbose=False,\n                      opset_version=19,\n                      input_names=['src', 'tgt'], \n                      output_names=['output'],\n                      dynamic_axes={\n                        \"src\": [0, 1],\n                        \"tgt\": [0, 1],\n                        \"output\": [0, 1]\n                      })\n\n############################### Prediction #######################################\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.tensor([[start_symbol]]).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n        ys = torch.cat([ys, torch.tensor([[next_word]]).to(DEVICE)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n \n\n# actual function to translate input sentence into target language\ndef translate(src_sentence: str):\n    transformer.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        transformer.module if DDP_ENABLED else transformer,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return (\"\" if TGT_LANGUAGE == 'zh' else \" \").join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n\nif not INFERENCE_ONLY and not IS_DATA_TRAINING_ITER_MODE:\n    test_ds_start = RANK * test_ds.shape[0] // WORLD_SIZE\n    test_ds_end = (RANK + 1) * test_ds.shape[0] // WORLD_SIZE\n    print(f'Test size: {test_ds_start} - {test_ds_end}')\n\n    if DDP_ENABLED:\n        torch.distributed.barrier()\n\n    for index, row in test_ds[test_ds_start: test_ds_end].iterrows():\n        print(f\"\"\"{row[SRC_LANGUAGE]}\n        {row[TGT_LANGUAGE]}\n        {translate(row[SRC_LANGUAGE])}\"\"\")\n\ndel print","metadata":{"execution":{"iopub.status.busy":"2024-10-20T09:49:46.045029Z","iopub.execute_input":"2024-10-20T09:49:46.046914Z","iopub.status.idle":"2024-10-20T09:49:46.087134Z","shell.execute_reply.started":"2024-10-20T09:49:46.046817Z","shell.execute_reply":"2024-10-20T09:49:46.08574Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!torchrun --standalone --nnodes=1 --nproc-per-node=2 ddp.py","metadata":{"execution":{"iopub.status.busy":"2024-10-16T16:16:43.581208Z","iopub.execute_input":"2024-10-16T16:16:43.581965Z","iopub.status.idle":"2024-10-16T16:19:12.116276Z","shell.execute_reply.started":"2024-10-16T16:16:43.58193Z","shell.execute_reply":"2024-10-16T16:19:12.115109Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onnxFileName = f'/kaggle/working/output/pytorch_translation_{os.getenv(\"SRC_LANGUAGE\", \"en\")}_{os.getenv(\"TGT_LANGUAGE\", \"en\")}.onnx'\nif os.path.exists(onnxFileName):\n#     !pip install onnx onnxruntime\n    import onnx\n    import onnxruntime as ort\n    import numpy as np\n    \n    # Load the ONNX model\n    model = onnx.load(onnxFileName)\n\n    # Check that the model is well formed\n    onnx.checker.check_model(model)\n\n    # Print a human readable representation of the graph\n#     print(onnx.helper.printable_graph(model.graph))\n\n    ort_session = ort.InferenceSession(onnxFileName)\n\n    outputs = ort_session.run(\n        None,\n        {'src': [[    2],\n            [12987],\n            [  143],\n            [ 1898],\n            [    3]], 'tgt': [[2], [0], [0], [0], [0]]},\n    )\n    print(outputs[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T16:02:43.219606Z","iopub.execute_input":"2024-10-16T16:02:43.219918Z","iopub.status.idle":"2024-10-16T16:02:43.227763Z","shell.execute_reply.started":"2024-10-16T16:02:43.219893Z","shell.execute_reply":"2024-10-16T16:02:43.226926Z"},"trusted":true},"outputs":[],"execution_count":null}]}